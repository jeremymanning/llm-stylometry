{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 1: Cross-entropy Loss Across Models and Authors\n",
    "\n",
    "This notebook generates Figure 1 from the paper, which shows:\n",
    "- **Part A**: Training curves showing cross-entropy loss over epochs for each model\n",
    "- **Part B**: Strip plot showing loss distributions per model on held-out test data\n",
    "\n",
    "As described in the paper:\n",
    "> \"We train a GPT-2 model on each author's corpus and use the trained model to compute the cross-entropy loss on held-out texts from both the target author and each of the other authors in the dataset. By comparing these losses, we assess whether the model captures author-specific stylistic patterns.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for publication-quality figures\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the consolidated model results\n",
    "data_path = Path('../data/model_results.pkl')\n",
    "df = pd.read_pickle(data_path)\n",
    "\n",
    "print(f\"Loaded {len(df):,} rows from {df['model_name'].nunique()} models\")\n",
    "print(f\"Authors: {sorted(df['author'].unique())}\")\n",
    "print(f\"Seeds: {sorted(df['seed'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Training Curves\n",
    "\n",
    "This plot shows the average cross-entropy loss on training data and held-out test data from each author, plotted as a function of the number of training epochs. Each color denotes a model trained on a single author's work. Error ribbons denote bootstrap-estimated 95% confidence intervals over 10 random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define standardized author colors and order\n",
    "AUTHOR_COLORS = {\n",
    "    \"baum\": \"#1f77b4\",\n",
    "    \"thompson\": \"#ff7f0e\",\n",
    "    \"austen\": \"#2ca02c\",\n",
    "    \"dickens\": \"#d62728\",\n",
    "    \"fitzgerald\": \"#9467bd\",\n",
    "    \"melville\": \"#8c564b\",\n",
    "    \"twain\": \"#e377c2\",\n",
    "    \"wells\": \"#7f7f7f\",\n",
    "}\n",
    "\n",
    "AUTHOR_ORDER = [\"baum\", \"thompson\", \"austen\", \"dickens\", \n",
    "                \"fitzgerald\", \"melville\", \"twain\", \"wells\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(df, max_epochs=1000, sample_every=10):\n",
    "    \"\"\"Plot training curves for all models.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot for training data and each evaluation author\n",
    "    datasets = ['train'] + AUTHOR_ORDER\n",
    "    \n",
    "    for idx, dataset in enumerate(datasets):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        for train_author in AUTHOR_ORDER:\n",
    "            # Filter data for this training author and dataset\n",
    "            subset = df[\n",
    "                (df['train_author'] == train_author) & \n",
    "                (df['loss_dataset'] == dataset) &\n",
    "                (df['epochs_completed'] <= max_epochs) &\n",
    "                (df['epochs_completed'] % sample_every == 0)  # Sample for performance\n",
    "            ]\n",
    "            \n",
    "            if len(subset) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate mean and confidence intervals\n",
    "            grouped = subset.groupby('epochs_completed')['loss_value']\n",
    "            mean_loss = grouped.mean()\n",
    "            std_loss = grouped.std()\n",
    "            count = grouped.count()\n",
    "            \n",
    "            # Calculate 95% CI\n",
    "            ci = 1.96 * std_loss / np.sqrt(count)\n",
    "            \n",
    "            # Plot\n",
    "            ax.plot(mean_loss.index, mean_loss.values, \n",
    "                   color=AUTHOR_COLORS[train_author], \n",
    "                   label=train_author.capitalize(), \n",
    "                   linewidth=1.5, alpha=0.8)\n",
    "            \n",
    "            # Add confidence interval\n",
    "            ax.fill_between(mean_loss.index, \n",
    "                           mean_loss.values - ci.values,\n",
    "                           mean_loss.values + ci.values,\n",
    "                           color=AUTHOR_COLORS[train_author], \n",
    "                           alpha=0.15)\n",
    "        \n",
    "        ax.set_title(f\"Evaluation: {dataset.capitalize()}\" if dataset != 'train' \n",
    "                    else \"Training Data\", fontsize=12)\n",
    "        ax.set_xlabel('Training Epochs', fontsize=10)\n",
    "        ax.set_ylabel('Cross-entropy Loss', fontsize=10)\n",
    "        ax.set_xlim(0, max_epochs)\n",
    "        ax.set_ylim(2.5, 5.5)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        if idx == 0:\n",
    "            ax.legend(loc='upper right', fontsize=8, ncol=2)\n",
    "    \n",
    "    # Hide the last subplot (we have 9 subplots but only 9 datasets)\n",
    "    plt.suptitle('Figure 1A: Cross-entropy Loss Across Training', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Figure 1A\n",
    "fig_1a = plot_training_curves(df, max_epochs=1000, sample_every=10)\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "output_path = Path('../paper/figs/source/all_losses_generated.pdf')\n",
    "fig_1a.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved Figure 1A to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Loss Distribution Strip Plot\n",
    "\n",
    "This plot shows the cross-entropy loss assigned to held-out test data by each author's model. Each point represents the loss for a specific held-out text, with colors indicating which author wrote the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_distributions(df):\n",
    "    \"\"\"Create strip plot of loss distributions.\"\"\"\n",
    "    \n",
    "    # Get final epoch losses for each model\n",
    "    final_losses = []\n",
    "    \n",
    "    for model_name in df['model_name'].unique():\n",
    "        model_df = df[df['model_name'] == model_name]\n",
    "        max_epoch = model_df['epochs_completed'].max()\n",
    "        final_epoch_df = model_df[\n",
    "            (model_df['epochs_completed'] == max_epoch) &\n",
    "            (model_df['loss_dataset'].isin(AUTHOR_ORDER))  # Only author datasets\n",
    "        ]\n",
    "        final_losses.append(final_epoch_df)\n",
    "    \n",
    "    final_df = pd.concat(final_losses, ignore_index=True)\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Create strip plot\n",
    "    for i, train_author in enumerate(AUTHOR_ORDER):\n",
    "        author_data = final_df[final_df['train_author'] == train_author]\n",
    "        \n",
    "        # Plot points for each evaluation author\n",
    "        for eval_author in AUTHOR_ORDER:\n",
    "            eval_data = author_data[author_data['loss_dataset'] == eval_author]\n",
    "            \n",
    "            if len(eval_data) > 0:\n",
    "                # Add jitter for visibility\n",
    "                x_positions = np.random.normal(i, 0.15, len(eval_data))\n",
    "                \n",
    "                ax.scatter(x_positions, eval_data['loss_value'].values,\n",
    "                          color=AUTHOR_COLORS[eval_author],\n",
    "                          alpha=0.6, s=30,\n",
    "                          label=eval_author.capitalize() if i == 0 else '')\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_xticks(range(len(AUTHOR_ORDER)))\n",
    "    ax.set_xticklabels([a.capitalize() for a in AUTHOR_ORDER], rotation=45, ha='right')\n",
    "    ax.set_xlabel('Model Trained On', fontsize=12)\n",
    "    ax.set_ylabel('Cross-entropy Loss', fontsize=12)\n",
    "    ax.set_title('Figure 1B: Loss Distributions by Model', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys(), \n",
    "             title='Evaluation Text Author',\n",
    "             bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Figure 1B\n",
    "fig_1b = plot_loss_distributions(df)\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "output_path = Path('../paper/figs/source/stripplot_generated.pdf')\n",
    "fig_1b.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved Figure 1B to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Observations\n",
    "\n",
    "From these plots, we can observe:\n",
    "\n",
    "1. **Training convergence**: All models show decreasing training loss over epochs, converging to different final values depending on the author.\n",
    "\n",
    "2. **Author-specific patterns**: Models achieve lower cross-entropy loss on texts from the author they were trained on (diagonal pattern in evaluation).\n",
    "\n",
    "3. **Stylistic similarity**: Some author pairs show lower cross-model losses, suggesting stylistic similarities.\n",
    "\n",
    "4. **Variance across seeds**: The confidence intervals show the consistency of results across different random initializations.\n",
    "\n",
    "These results support the paper's claim that \"a model trained on a given author should exhibit lower loss when predicting that author's own texts as compared to the texts of others.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display some summary statistics\n",
    "final_epoch_data = df[df['epochs_completed'] == df.groupby('model_name')['epochs_completed'].transform('max')]\n",
    "\n",
    "print(\"Average loss by model on same-author vs different-author texts:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for train_author in AUTHOR_ORDER:\n",
    "    author_models = final_epoch_data[final_epoch_data['train_author'] == train_author]\n",
    "    \n",
    "    same_author = author_models[author_models['loss_dataset'] == train_author]['loss_value'].mean()\n",
    "    diff_author = author_models[author_models['loss_dataset'] != train_author]['loss_value'].mean()\n",
    "    \n",
    "    print(f\"{train_author.capitalize():12} - Same: {same_author:.3f}, Different: {diff_author:.3f}, \"\n",
    "          f\"Difference: {diff_author - same_author:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}